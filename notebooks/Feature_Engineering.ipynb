{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d810824a",
   "metadata": {},
   "source": [
    "# Feature Engineering for AQI Prediction\n",
    "\n",
    "This notebook compares historical and current data, then creates features for AQI prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827a28c",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5b514b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: d:\\Internships and Jobs Data\\10 Pearls Shine Internship\\Project\\AQI Predictor\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup paths\n",
    "BASE_DIR = Path.cwd().parent\n",
    "RAW_DATA_PATH = BASE_DIR / 'data' / 'raw'\n",
    "\n",
    "print(f'Data directory: {RAW_DATA_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e4d8a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cda775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical Weather Data:\n",
      "Shape: (2208, 8)\n",
      "Columns: ['timestamp', 'temperature', 'humidity', 'dew_point', 'precipitation', 'wind_speed', 'wind_direction', 'pressure']\n",
      "\n",
      "Historical Pollution Data:\n",
      "Shape: (2137, 10)\n",
      "Columns: ['timestamp', 'aqi', 'co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']\n",
      "\n",
      "Current Weather Data:\n",
      "Shape: (2, 17)\n",
      "Columns: ['timestamp', 'city', 'country', 'latitude', 'longitude', 'temperature', 'feels_like', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'cloudiness', 'visibility', 'weather_main', 'weather_description']\n",
      "\n",
      "Current Pollution Data:\n",
      "Shape: (2, 10)\n",
      "Columns: ['timestamp', 'aqi', 'co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load historical data\n",
    "historical_weather = pd.read_csv(RAW_DATA_PATH / 'historical_weather_data.csv')\n",
    "historical_pollution = pd.read_csv(RAW_DATA_PATH / 'historical_pollution_data.csv')\n",
    "\n",
    "# Load current data\n",
    "current_weather = pd.read_csv(RAW_DATA_PATH / 'current_weather_data.csv')\n",
    "current_pollution = pd.read_csv(RAW_DATA_PATH / 'current_pollution_data.csv')\n",
    "\n",
    "print('Historical Weather Data:')\n",
    "print(f'Shape: {historical_weather.shape}')\n",
    "print(f'Columns: {list(historical_weather.columns)}')\n",
    "print()\n",
    "\n",
    "print('Historical Pollution Data:')\n",
    "print(f'Shape: {historical_pollution.shape}')\n",
    "print(f'Columns: {list(historical_pollution.columns)}')\n",
    "print()\n",
    "\n",
    "print('Current Weather Data:')\n",
    "print(f'Shape: {current_weather.shape}')\n",
    "print(f'Columns: {list(current_weather.columns)}')\n",
    "print()\n",
    "\n",
    "print('Current Pollution Data:')\n",
    "print(f'Shape: {current_pollution.shape}')\n",
    "print(f'Columns: {list(current_pollution.columns)}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997984a7",
   "metadata": {},
   "source": [
    "## Compare Column Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7c6e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COLUMN COMPARISON\n",
      "================================================================================\n",
      "\n",
      "1. Historical Weather Columns:\n",
      "['timestamp', 'temperature', 'humidity', 'dew_point', 'precipitation', 'wind_speed', 'wind_direction', 'pressure']\n",
      "\n",
      "2. Current Weather Columns:\n",
      "['timestamp', 'city', 'country', 'latitude', 'longitude', 'temperature', 'feels_like', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'cloudiness', 'visibility', 'weather_main', 'weather_description']\n",
      "\n",
      "3. Common Weather Columns:\n",
      "['humidity', 'wind_speed', 'temperature', 'pressure', 'timestamp']\n",
      "\n",
      "4. Missing in Current Weather (but in Historical):\n",
      "['precipitation', 'wind_direction', 'dew_point']\n",
      "\n",
      "5. Extra in Current Weather (not in Historical):\n",
      "['longitude', 'feels_like', 'city', 'visibility', 'latitude', 'weather_description', 'temp_max', 'country', 'weather_main', 'wind_deg', 'temp_min', 'cloudiness']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "6. Historical Pollution Columns:\n",
      "['timestamp', 'aqi', 'co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']\n",
      "\n",
      "7. Current Pollution Columns:\n",
      "['timestamp', 'aqi', 'co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']\n",
      "\n",
      "8. Common Pollution Columns:\n",
      "['aqi', 'so2', 'no', 'co', 'no2', 'pm2_5', 'o3', 'pm10', 'nh3', 'timestamp']\n",
      "\n",
      "9. Differences in Pollution Columns:\n",
      "Missing in Current: []\n",
      "Extra in Current: []\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('COLUMN COMPARISON')\n",
    "print('='*80)\n",
    "\n",
    "print('\\n1. Historical Weather Columns:')\n",
    "print(list(historical_weather.columns))\n",
    "\n",
    "print('\\n2. Current Weather Columns:')\n",
    "print(list(current_weather.columns))\n",
    "\n",
    "print('\\n3. Common Weather Columns:')\n",
    "hist_weather_cols = set(historical_weather.columns)\n",
    "curr_weather_cols = set(current_weather.columns)\n",
    "common_weather = hist_weather_cols.intersection(curr_weather_cols)\n",
    "print(list(common_weather))\n",
    "\n",
    "print('\\n4. Missing in Current Weather (but in Historical):')\n",
    "print(list(hist_weather_cols - curr_weather_cols))\n",
    "\n",
    "print('\\n5. Extra in Current Weather (not in Historical):')\n",
    "print(list(curr_weather_cols - hist_weather_cols))\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "print('\\n6. Historical Pollution Columns:')\n",
    "print(list(historical_pollution.columns))\n",
    "\n",
    "print('\\n7. Current Pollution Columns:')\n",
    "print(list(current_pollution.columns))\n",
    "\n",
    "print('\\n8. Common Pollution Columns:')\n",
    "hist_poll_cols = set(historical_pollution.columns)\n",
    "curr_poll_cols = set(current_pollution.columns)\n",
    "common_pollution = hist_poll_cols.intersection(curr_poll_cols)\n",
    "print(list(common_pollution))\n",
    "\n",
    "print('\\n9. Differences in Pollution Columns:')\n",
    "print(f'Missing in Current: {list(hist_poll_cols - curr_poll_cols)}')\n",
    "print(f'Extra in Current: {list(curr_poll_cols - hist_poll_cols)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb95b7",
   "metadata": {},
   "source": [
    "## Data Type and Value Range Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "667d4316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA STATISTICS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "HISTORICAL WEATHER STATISTICS:\n",
      "       temperature     humidity   wind_speed     pressure\n",
      "count  2208.000000  2208.000000  2208.000000  2208.000000\n",
      "mean     19.661096    52.004181     7.469834  1013.411133\n",
      "std       5.579398    21.094570     3.715751     3.118493\n",
      "min       7.300000     9.499662     0.000000  1003.065100\n",
      "25%      15.450000    34.899327     4.599695  1011.606550\n",
      "50%      19.100000    50.808365     7.558927  1013.453750\n",
      "75%      23.700000    69.114810    10.239453  1015.451610\n",
      "max      35.950000    98.155650    24.412407  1022.263060\n",
      "\n",
      "CURRENT WEATHER STATISTICS:\n",
      "       temperature   humidity  wind_speed     pressure\n",
      "count     2.000000   2.000000    2.000000     2.000000\n",
      "mean     23.775000  22.000000    3.475000  1019.000000\n",
      "std       2.524371   2.828427    0.883883     1.414214\n",
      "min      21.990000  20.000000    2.850000  1018.000000\n",
      "25%      22.882500  21.000000    3.162500  1018.500000\n",
      "50%      23.775000  22.000000    3.475000  1019.000000\n",
      "75%      24.667500  23.000000    3.787500  1019.500000\n",
      "max      25.560000  24.000000    4.100000  1020.000000\n",
      "\n",
      "================================================================================\n",
      "\n",
      "HISTORICAL POLLUTION STATISTICS:\n",
      "               aqi        pm2_5         pm10           co          no2  \\\n",
      "count  2137.000000  2137.000000  2137.000000  2137.000000  2137.000000   \n",
      "mean      4.187178    81.543088   152.790819   464.390599     5.100889   \n",
      "std       0.882924    52.880661    82.182711   241.805284     3.683738   \n",
      "min       2.000000     5.730000    15.880000   116.610000     0.350000   \n",
      "25%       3.000000    42.910000    91.720000   303.630000     2.250000   \n",
      "50%       4.000000    72.070000   141.720000   408.580000     4.090000   \n",
      "75%       5.000000   104.480000   195.990000   568.720000     6.970000   \n",
      "max       5.000000   321.820000   497.150000  1650.490000    18.950000   \n",
      "\n",
      "                o3  \n",
      "count  2137.000000  \n",
      "mean     98.220229  \n",
      "std      30.616745  \n",
      "min      29.020000  \n",
      "25%      73.140000  \n",
      "50%      99.450000  \n",
      "75%     122.670000  \n",
      "max     165.500000  \n",
      "\n",
      "CURRENT POLLUTION STATISTICS:\n",
      "       aqi     pm2_5        pm10          co       no2          o3\n",
      "count  2.0   2.00000    2.000000    2.000000  2.000000    2.000000\n",
      "mean   4.0  67.98500  116.175000  468.630000  8.015000  124.115000\n",
      "std    0.0   0.53033    1.237437   87.440825  2.552655   23.525443\n",
      "min    4.0  67.61000  115.300000  406.800000  6.210000  107.480000\n",
      "25%    4.0  67.79750  115.737500  437.715000  7.112500  115.797500\n",
      "50%    4.0  67.98500  116.175000  468.630000  8.015000  124.115000\n",
      "75%    4.0  68.17250  116.612500  499.545000  8.917500  132.432500\n",
      "max    4.0  68.36000  117.050000  530.460000  9.820000  140.750000\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('DATA STATISTICS COMPARISON')\n",
    "print('='*80)\n",
    "\n",
    "# Compare weather data\n",
    "print('\\nHISTORICAL WEATHER STATISTICS:')\n",
    "print(historical_weather[['temperature', 'humidity', 'wind_speed', 'pressure']].describe())\n",
    "\n",
    "print('\\nCURRENT WEATHER STATISTICS:')\n",
    "if 'temperature' in current_weather.columns:\n",
    "    print(current_weather[['temperature', 'humidity', 'wind_speed', 'pressure']].describe())\n",
    "\n",
    "# Compare pollution data\n",
    "print('\\n' + '='*80)\n",
    "print('\\nHISTORICAL POLLUTION STATISTICS:')\n",
    "print(historical_pollution[['aqi', 'pm2_5', 'pm10', 'co', 'no2', 'o3']].describe())\n",
    "\n",
    "print('\\nCURRENT POLLUTION STATISTICS:')\n",
    "print(current_pollution[['aqi', 'pm2_5', 'pm10', 'co', 'no2', 'o3']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779ab44",
   "metadata": {},
   "source": [
    "## Check Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18521203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HISTORICAL WEATHER - First 3 rows:\n",
      "                   timestamp  temperature   humidity  dew_point  \\\n",
      "0  2025-11-01 00:00:00+00:00        22.45  53.880460      12.65   \n",
      "1  2025-11-01 01:00:00+00:00        21.35  56.498665      12.35   \n",
      "2  2025-11-01 02:00:00+00:00        21.20  62.887740      13.85   \n",
      "\n",
      "   precipitation  wind_speed  wind_direction   pressure  \n",
      "0            0.0    1.548418      125.537766  1006.7212  \n",
      "1            0.0    0.360000      270.000000  1007.0077  \n",
      "2            0.0    0.569210      341.564940  1008.1024  \n",
      "\n",
      "CURRENT WEATHER - First row:\n",
      "                   timestamp       city country  latitude  longitude  \\\n",
      "0  2026-02-05 12:56:51+00:00  HyderÄbÄd      PK    25.396    68.3578   \n",
      "\n",
      "   temperature  feels_like  temp_min  temp_max  pressure  humidity  \\\n",
      "0        25.56       24.69     25.56     25.56      1018        20   \n",
      "\n",
      "   wind_speed  wind_deg  cloudiness  visibility weather_main  \\\n",
      "0        2.85         2           0       10000        Clear   \n",
      "\n",
      "  weather_description  \n",
      "0           clear sky  \n",
      "\n",
      "HISTORICAL POLLUTION - First 3 rows:\n",
      "                   timestamp  aqi      co   no   no2     o3   so2  pm2_5  \\\n",
      "0  2025-10-31 19:00:00+00:00    5  469.14  0.0  5.76  90.53  4.86  76.24   \n",
      "1  2025-10-31 20:00:00+00:00    5  474.46  0.0  5.48  80.74  3.73  75.22   \n",
      "2  2025-10-31 21:00:00+00:00    4  477.31  0.0  4.92  71.45  2.70  74.28   \n",
      "\n",
      "     pm10   nh3  \n",
      "0  126.14  4.67  \n",
      "1  122.29  4.85  \n",
      "2  119.34  4.93  \n",
      "\n",
      "CURRENT POLLUTION - First row:\n",
      "                   timestamp  aqi     co    no   no2      o3   so2  pm2_5  \\\n",
      "0  2026-02-05 12:56:49+00:00    4  406.8  0.06  6.21  140.75  6.21  67.61   \n",
      "\n",
      "     pm10   nh3  \n",
      "0  117.05  7.37  \n"
     ]
    }
   ],
   "source": [
    "print('HISTORICAL WEATHER - First 3 rows:')\n",
    "print(historical_weather.head(3))\n",
    "print()\n",
    "\n",
    "print('CURRENT WEATHER - First row:')\n",
    "print(current_weather.head(1))\n",
    "print()\n",
    "\n",
    "print('HISTORICAL POLLUTION - First 3 rows:')\n",
    "print(historical_pollution.head(3))\n",
    "print()\n",
    "\n",
    "print('CURRENT POLLUTION - First row:')\n",
    "print(current_pollution.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f876efa",
   "metadata": {},
   "source": [
    "## Identify Issues and Required Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1459d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ALIGNMENT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. Required Pollution Features:\n",
      "  âœ“ aqi             - Historical: True, Current: True\n",
      "  âœ“ co              - Historical: True, Current: True\n",
      "  âœ“ no              - Historical: True, Current: True\n",
      "  âœ“ no2             - Historical: True, Current: True\n",
      "  âœ“ o3              - Historical: True, Current: True\n",
      "  âœ“ so2             - Historical: True, Current: True\n",
      "  âœ“ pm2_5           - Historical: True, Current: True\n",
      "  âœ“ pm10            - Historical: True, Current: True\n",
      "  âœ“ nh3             - Historical: True, Current: True\n",
      "\n",
      "2. Required Weather Features:\n",
      "  âœ“ temperature     - Historical: True, Current: True\n",
      "  âœ“ humidity        - Historical: True, Current: True\n",
      "  âœ“ wind_speed      - Historical: True, Current: True\n",
      "  âœ— precipitation   - Historical: True, Current: False\n",
      "  âœ“ pressure        - Historical: True, Current: True\n",
      "\n",
      "3. Issues Found:\n",
      "  âš  Current weather missing precipitation (not available in OpenWeather current API)\n",
      "  â„¹ Current weather has extra columns: ['longitude', 'feels_like', 'city', 'visibility', 'latitude', 'weather_description', 'temp_max', 'country', 'weather_main', 'wind_deg', 'temp_min', 'cloudiness']\n",
      "  âš  Wind direction column names differ: wind_direction vs wind_deg\n",
      "  âš  Current weather missing dew_point\n",
      "\n",
      "4. Recommendations:\n",
      "  â€¢ Rename wind_deg â†’ wind_direction in current weather\n",
      "  â€¢ Add precipitation=0 as default for current weather (not raining if not in data)\n",
      "  â€¢ Calculate dew_point from temperature & humidity if needed\n",
      "  â€¢ Drop extra columns from current weather that are not in historical data\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('ALIGNMENT ANALYSIS')\n",
    "print('='*80)\n",
    "\n",
    "# Key features needed for AQI prediction\n",
    "required_features = {\n",
    "    'pollution': ['aqi', 'co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3'],\n",
    "    'weather': ['temperature', 'humidity', 'wind_speed', 'precipitation', 'pressure'],\n",
    "    'temporal': ['timestamp']\n",
    "}\n",
    "\n",
    "print('\\n1. Required Pollution Features:')\n",
    "for feat in required_features['pollution']:\n",
    "    hist_has = feat in historical_pollution.columns\n",
    "    curr_has = feat in current_pollution.columns\n",
    "    status = 'âœ“' if (hist_has and curr_has) else 'âœ—'\n",
    "    print(f'  {status} {feat:15s} - Historical: {hist_has}, Current: {curr_has}')\n",
    "\n",
    "print('\\n2. Required Weather Features:')\n",
    "for feat in required_features['weather']:\n",
    "    hist_has = feat in historical_weather.columns\n",
    "    curr_has = feat in current_weather.columns\n",
    "    status = 'âœ“' if (hist_has and curr_has) else 'âœ—'\n",
    "    print(f'  {status} {feat:15s} - Historical: {hist_has}, Current: {curr_has}')\n",
    "\n",
    "print('\\n3. Issues Found:')\n",
    "issues = []\n",
    "\n",
    "# Check for precipitation\n",
    "if 'precipitation' in historical_weather.columns and 'precipitation' not in current_weather.columns:\n",
    "    issues.append('âš  Current weather missing precipitation (not available in OpenWeather current API)')\n",
    "\n",
    "# Check for extra columns in current\n",
    "extra_current_weather = curr_weather_cols - hist_weather_cols - {'timestamp'}\n",
    "if extra_current_weather:\n",
    "    issues.append(f'â„¹ Current weather has extra columns: {list(extra_current_weather)}')\n",
    "\n",
    "# Check for wind_direction\n",
    "if 'wind_direction' in historical_weather.columns and 'wind_deg' in current_weather.columns:\n",
    "    issues.append('âš  Wind direction column names differ: wind_direction vs wind_deg')\n",
    "\n",
    "# Check for dew_point\n",
    "if 'dew_point' in historical_weather.columns and 'dew_point' not in current_weather.columns:\n",
    "    issues.append('âš  Current weather missing dew_point')\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f'  {issue}')\n",
    "else:\n",
    "    print('  âœ“ No critical issues found')\n",
    "\n",
    "print('\\n4. Recommendations:')\n",
    "print('  â€¢ Rename wind_deg â†’ wind_direction in current weather')\n",
    "print('  â€¢ Add precipitation=0 as default for current weather (not raining if not in data)')\n",
    "print('  â€¢ Calculate dew_point from temperature & humidity if needed')\n",
    "print('  â€¢ Drop extra columns from current weather that are not in historical data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1470cb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Engineering Discussion\n",
    "\n",
    "Now let's discuss what features we need to predict AQI for the next few days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e3ba1",
   "metadata": {},
   "source": [
    "### Feature Categories for AQI Prediction\n",
    "\n",
    "Based on your sample features, here's an analysis:\n",
    "\n",
    "**1. TEMPORAL FEATURES** (Essential)\n",
    "- `year`, `month`, `day`, `hour`, `weekday` â†’ Capture time patterns\n",
    "- `season` â†’ Air quality varies by season\n",
    "- `is_weekend` â†’ Traffic patterns differ on weekends\n",
    "- `time_of_day` â†’ Morning/evening pollution peaks\n",
    "- **Why needed?** AQI has strong temporal patterns (daily cycles, seasonal changes)\n",
    "\n",
    "**2. RAW POLLUTION FEATURES** (Core Features)\n",
    "- `co`, `no`, `no2`, `o3`, `so2`, `pm2_5`, `pm10`, `nh3`\n",
    "- **Why needed?** AQI is calculated from these pollutants. They are the most important predictors.\n",
    "\n",
    "**3. RAW WEATHER FEATURES** (Important)\n",
    "- `temperature`, `humidity`, `wind_speed`, `precipitation`\n",
    "- **Why needed?** Weather affects pollutant dispersion. Wind disperses pollution, rain washes it away, temperature inversions trap pollution.\n",
    "\n",
    "**4. LAG FEATURES** (Very Important for Time Series)\n",
    "- `aqi_lag_1`, `co_lag_1`, `no2_lag_1`, `pm2_5_lag_1`, `pm10_lag_1`\n",
    "- **Why needed?** Air quality at time T depends heavily on air quality at T-1. These capture momentum/persistence.\n",
    "- **Recommendation:** Also add lag_2, lag_3, lag_6, lag_12, lag_24 for better patterns\n",
    "\n",
    "**5. ROLLING WINDOW FEATURES** (Important)\n",
    "- `aqi_rolling_std`, `aqi_rolling_min`, `aqi_rolling_max`\n",
    "- `co_rolling_avg`, `co_rolling_std`, `no2_rolling_avg`, etc.\n",
    "- **Why needed?** Capture recent trends and volatility. 24-hour rolling mean shows daily average trend.\n",
    "- **Recommendation:** Use 6h, 12h, and 24h windows\n",
    "\n",
    "**6. CHANGE RATE FEATURES** (Useful)\n",
    "- `co_change_rate`, `no2_change_rate`, `pm2_5_change_rate`, etc.\n",
    "- `temperature_change_rate`, `humidity_change_rate`, `wind_speed_change_rate`\n",
    "- **Why needed?** Rate of change indicates if pollution is increasing or decreasing.\n",
    "- **Note:** Might be redundant if we have lag features, but can help.\n",
    "\n",
    "**7. INTERACTION FEATURES** (Moderately Useful)\n",
    "- `temp_humidity_interaction` = temperature Ã— humidity\n",
    "- `wind_pm2_5_interaction` = wind_speed Ã— pm2_5\n",
    "- **Why needed?** High humidity + low temperature traps pollution. Strong wind reduces PM2.5 impact.\n",
    "- **Recommendation:** Add wind_speed Ã— temperature (thermal mixing)\n",
    "\n",
    "**8. CUMULATIVE FEATURES** (Less Important)\n",
    "- `cumulative_precipitation`, `cumulative_co`, `cumulative_no2`, etc.\n",
    "- **Why needed?** Shows accumulated pollution over time.\n",
    "- **Note:** May not be very useful since we're predicting future AQI, not analyzing long-term exposure.\n",
    "- **Recommendation:** Skip these or use rolling sums instead.\n",
    "\n",
    "**9. ALERT/BINARY FEATURES** (Useful)\n",
    "- `high_pollution_alert` â†’ Binary flag for AQI > threshold\n",
    "- `rain_alert` â†’ Binary flag for precipitation > 0\n",
    "- **Why needed?** Helps model learn thresholds and weather events.\n",
    "\n",
    "**10. POLYNOMIAL FEATURES** (Optional)\n",
    "- `temperature_squared`, `humidity_squared`\n",
    "- **Why needed?** Capture non-linear relationships.\n",
    "- **Note:** Tree-based models (XGBoost, Random Forest) don't need these. Neural networks might benefit.\n",
    "- **Recommendation:** Skip for tree models, add for linear/neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40b5b2",
   "metadata": {},
   "source": [
    "### Recommended Feature Set for Your AQI Predictor\n",
    "\n",
    "**Priority 1 - Must Have (Core):**\n",
    "```\n",
    "- Temporal: year, month, day, hour, weekday, is_weekend, season, time_of_day\n",
    "- Pollution: co, no2, o3, so2, pm2_5, pm10\n",
    "- Weather: temperature, humidity, wind_speed, pressure\n",
    "- Lag (1-24 hours): aqi_lag_1, pm2_5_lag_1, pm10_lag_1, co_lag_1\n",
    "```\n",
    "\n",
    "**Priority 2 - Should Have (Performance Boost):**\n",
    "```\n",
    "- Rolling (6h, 12h, 24h): aqi_rolling_mean, pm2_5_rolling_mean, pm2_5_rolling_std\n",
    "- More lags: aqi_lag_6, aqi_lag_12, aqi_lag_24\n",
    "- Change rates: pm2_5_change_rate, temperature_change_rate\n",
    "```\n",
    "\n",
    "**Priority 3 - Nice to Have (Marginal Gains):**\n",
    "```\n",
    "- Interactions: temp_humidity_interaction, wind_pm2_5_interaction\n",
    "- Alerts: high_pollution_alert, rain_alert\n",
    "- Additional rolling: co_rolling_mean, no2_rolling_mean\n",
    "```\n",
    "\n",
    "**Skip (Not Worth It):**\n",
    "```\n",
    "- Cumulative features (unless analyzing long-term trends)\n",
    "- Squared features (if using tree-based models)\n",
    "- no, nh3 (less correlated with AQI)\n",
    "- precipitation (not available in current OpenWeather API)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efa2f8",
   "metadata": {},
   "source": [
    "### Target Variable: What Are We Predicting?\n",
    "\n",
    "**For \"next few days\" prediction, we need to decide:**\n",
    "\n",
    "1. **Single-step prediction:** Predict AQI for next hour\n",
    "   - Input: Current features + lags\n",
    "   - Output: AQI at t+1\n",
    "   - Easier to train, more accurate\n",
    "\n",
    "2. **Multi-step prediction:** Predict AQI for next 24 hours\n",
    "   - Input: Current features + lags  \n",
    "   - Output: [AQI at t+1, t+2, ..., t+24]\n",
    "   - Harder but more useful\n",
    "\n",
    "3. **Multi-horizon prediction:** Predict daily average AQI for next 3-7 days\n",
    "   - Input: Current features + lags\n",
    "   - Output: Daily average AQI for next 3-7 days\n",
    "   - Most practical for \"next few days\"\n",
    "\n",
    "**Recommendation:**\n",
    "- Start with **single-step** (predict next hour) to build baseline\n",
    "- Then try **multi-step** (predict next 24-48 hours)\n",
    "- For dashboard: Show hourly predictions for next 24h + daily averages for next 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97ac34",
   "metadata": {},
   "source": [
    "### Action Plan\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. **Align column names** between historical and current data\n",
    "   - Rename `wind_deg` â†’ `wind_direction`\n",
    "   - Add missing columns with default values\n",
    "\n",
    "2. **Merge historical data** (weather + pollution by timestamp)\n",
    "\n",
    "3. **Create features** in this order:\n",
    "   - Temporal features (year, month, day, hour, etc.)\n",
    "   - Lag features (1, 6, 12, 24 hours)\n",
    "   - Rolling features (6h, 12h, 24h windows)\n",
    "   - Change rate features\n",
    "   - Interaction features\n",
    "   - Alert features\n",
    "\n",
    "4. **Handle missing values**\n",
    "   - Forward fill for short gaps (< 3 hours)\n",
    "   - Drop rows with too many missing values\n",
    "\n",
    "5. **Save to feature store** (Hopsworks or MongoDB Atlas)\n",
    "\n",
    "6. **Train models**\n",
    "   - XGBoost, LightGBM, Random Forest\n",
    "   - Compare performance\n",
    "\n",
    "7. **Deploy** prediction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad3a99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of Comparison Results\n",
    "\n",
    "Run the cells above to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86f98f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "âœ“ Comparison cells created above will show:\n",
      "  1. Column structure differences between historical and current data\n",
      "  2. Data type and value range comparisons\n",
      "  3. Sample data from both sources\n",
      "  4. Alignment issues and recommendations\n",
      "\n",
      "âœ“ Feature engineering discussion completed:\n",
      "  1. Identified essential features for AQI prediction\n",
      "  2. Prioritized features by importance\n",
      "  3. Recommended skipping unnecessary features\n",
      "  4. Defined target variable options (single-step vs multi-step)\n",
      "\n",
      "ðŸ“Œ Next: Run all cells above to see the actual comparison results!\n",
      "ðŸ“Œ Then: Decide on final feature set based on comparison findings\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('SUMMARY')\n",
    "print('='*80)\n",
    "print()\n",
    "print('âœ“ Comparison cells created above will show:')\n",
    "print('  1. Column structure differences between historical and current data')\n",
    "print('  2. Data type and value range comparisons')\n",
    "print('  3. Sample data from both sources')\n",
    "print('  4. Alignment issues and recommendations')\n",
    "print()\n",
    "print('âœ“ Feature engineering discussion completed:')\n",
    "print('  1. Identified essential features for AQI prediction')\n",
    "print('  2. Prioritized features by importance')\n",
    "print('  3. Recommended skipping unnecessary features')\n",
    "print('  4. Defined target variable options (single-step vs multi-step)')\n",
    "print()\n",
    "print('ðŸ“Œ Next: Run all cells above to see the actual comparison results!')\n",
    "print('ðŸ“Œ Then: Decide on final feature set based on comparison findings')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758a3df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Align Column Names\n",
    "\n",
    "Normalize column names between historical and current data to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da6b3c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning current weather data...\n",
      "âœ“ Renamed wind_deg â†’ wind_direction\n",
      "âœ“ Added precipitation column (default: 0.0)\n",
      "âœ“ Calculated dew_point from temperature and humidity\n",
      "\n",
      "âœ“ Current weather aligned: (2, 8)\n",
      "  Columns: ['timestamp', 'temperature', 'humidity', 'dew_point', 'precipitation', 'wind_speed', 'wind_direction', 'pressure']\n",
      "\n",
      "âœ“ Pollution data already aligned (same columns in both)\n",
      "  Historical pollution: (2137, 10)\n",
      "  Current pollution: (2, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create copies to avoid modifying original data\n",
    "hist_weather_clean = historical_weather.copy()\n",
    "hist_pollution_clean = historical_pollution.copy()\n",
    "curr_weather_clean = current_weather.copy()\n",
    "curr_pollution_clean = current_pollution.copy()\n",
    "\n",
    "# Align current weather columns with historical\n",
    "print('Aligning current weather data...')\n",
    "\n",
    "# Rename wind_deg to wind_direction\n",
    "if 'wind_deg' in curr_weather_clean.columns:\n",
    "    curr_weather_clean.rename(columns={'wind_deg': 'wind_direction'}, inplace=True)\n",
    "    print('âœ“ Renamed wind_deg â†’ wind_direction')\n",
    "\n",
    "# Add missing columns with default values\n",
    "if 'precipitation' not in curr_weather_clean.columns:\n",
    "    curr_weather_clean['precipitation'] = 0.0\n",
    "    print('âœ“ Added precipitation column (default: 0.0)')\n",
    "\n",
    "if 'dew_point' not in curr_weather_clean.columns:\n",
    "    # Calculate dew point from temperature and humidity using Magnus formula\n",
    "    # Td = T - ((100 - RH) / 5)  (simplified approximation)\n",
    "    curr_weather_clean['dew_point'] = curr_weather_clean['temperature'] - ((100 - curr_weather_clean['humidity']) / 5)\n",
    "    print('âœ“ Calculated dew_point from temperature and humidity')\n",
    "\n",
    "# Select only columns that exist in historical data\n",
    "hist_weather_cols = list(hist_weather_clean.columns)\n",
    "curr_weather_cols_to_keep = [col for col in hist_weather_cols if col in curr_weather_clean.columns or col == 'timestamp']\n",
    "\n",
    "# Keep only necessary columns\n",
    "curr_weather_clean = curr_weather_clean[curr_weather_cols_to_keep]\n",
    "\n",
    "print(f'\\nâœ“ Current weather aligned: {curr_weather_clean.shape}')\n",
    "print(f'  Columns: {list(curr_weather_clean.columns)}')\n",
    "\n",
    "print('\\nâœ“ Pollution data already aligned (same columns in both)')\n",
    "print(f'  Historical pollution: {hist_pollution_clean.shape}')\n",
    "print(f'  Current pollution: {curr_pollution_clean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdca05",
   "metadata": {},
   "source": [
    "## Step 2: Merge Historical Data\n",
    "\n",
    "Merge historical weather and pollution data by timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9220072d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Merged historical data: (2132, 17)\n",
      "  Date range: 2025-11-01 00:00:00+00:00 to 2026-01-30 19:00:00+00:00\n",
      "  Total records: 2132\n",
      "\n",
      "Columns: ['timestamp', 'temperature', 'humidity', 'dew_point', 'precipitation', 'wind_speed', 'wind_direction', 'pressure', 'aqi', 'co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']\n",
      "\n",
      "âœ“ No missing values in merged data\n",
      "\n",
      "Sample of merged data:\n",
      "                  timestamp  temperature   humidity  dew_point  precipitation  \\\n",
      "0 2025-11-01 00:00:00+00:00        22.45  53.880460      12.65            0.0   \n",
      "1 2025-11-01 01:00:00+00:00        21.35  56.498665      12.35            0.0   \n",
      "2 2025-11-01 02:00:00+00:00        21.20  62.887740      13.85            0.0   \n",
      "\n",
      "   wind_speed  wind_direction   pressure  aqi      co   no   no2     o3   so2  \\\n",
      "0    1.548418      125.537766  1006.7212    5  515.33  0.0  4.06  56.59  1.41   \n",
      "1    0.360000      270.000000  1007.0077    5  526.95  0.0  3.99  54.78  1.31   \n",
      "2    0.569210      341.564940  1008.1024    5  529.91  0.0  4.01  54.87  1.36   \n",
      "\n",
      "   pm2_5    pm10   nh3  \n",
      "0  76.72  127.27  5.72  \n",
      "1  77.65  132.63  6.44  \n",
      "2  78.32  137.79  6.55  \n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp columns to datetime\n",
    "hist_weather_clean['timestamp'] = pd.to_datetime(hist_weather_clean['timestamp'])\n",
    "hist_pollution_clean['timestamp'] = pd.to_datetime(hist_pollution_clean['timestamp'])\n",
    "\n",
    "# Merge on timestamp\n",
    "historical_merged = pd.merge(\n",
    "    hist_weather_clean,\n",
    "    hist_pollution_clean,\n",
    "    on='timestamp',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f'âœ“ Merged historical data: {historical_merged.shape}')\n",
    "print(f'  Date range: {historical_merged[\"timestamp\"].min()} to {historical_merged[\"timestamp\"].max()}')\n",
    "print(f'  Total records: {len(historical_merged)}')\n",
    "print(f'\\nColumns: {list(historical_merged.columns)}')\n",
    "\n",
    "# Check for missing values\n",
    "missing_counts = historical_merged.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(f'\\nâš  Missing values found:')\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "else:\n",
    "    print('\\nâœ“ No missing values in merged data')\n",
    "\n",
    "# Display sample\n",
    "print('\\nSample of merged data:')\n",
    "print(historical_merged.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9bda9",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "\n",
    "Create temporal, lag, rolling, change rate, interaction, and alert features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebee464",
   "metadata": {},
   "source": [
    "### 3.1 Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3002db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created temporal features:\n",
      "  year, month, day, hour, weekday, is_weekend, season, time_of_day\n",
      "\n",
      "Sample:\n",
      "                  timestamp  year  month  day  hour  weekday  is_weekend  \\\n",
      "0 2025-11-01 00:00:00+00:00  2025     11    1     0        5           1   \n",
      "1 2025-11-01 01:00:00+00:00  2025     11    1     1        5           1   \n",
      "2 2025-11-01 02:00:00+00:00  2025     11    1     2        5           1   \n",
      "\n",
      "   season  time_of_day  \n",
      "0       4            0  \n",
      "1       4            0  \n",
      "2       4            0  \n"
     ]
    }
   ],
   "source": [
    "def create_temporal_features(df):\n",
    "    \"\"\"Create temporal features from timestamp\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['year'] = df['timestamp'].dt.year\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['weekday'] = df['timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "    \n",
    "    # Season (1=Winter, 2=Spring, 3=Summer, 4=Fall for Northern Hemisphere)\n",
    "    # Pakistan: Dec-Feb=Winter, Mar-May=Spring, Jun-Aug=Summer, Sep-Nov=Fall\n",
    "    df['season'] = df['month'].apply(lambda m: \n",
    "        1 if m in [12, 1, 2] else \n",
    "        2 if m in [3, 4, 5] else \n",
    "        3 if m in [6, 7, 8] else 4\n",
    "    )\n",
    "    \n",
    "    # Time of day (0=Night, 1=Morning, 2=Afternoon, 3=Evening)\n",
    "    df['time_of_day'] = df['hour'].apply(lambda h:\n",
    "        0 if h < 6 else\n",
    "        1 if h < 12 else\n",
    "        2 if h < 18 else 3\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to historical data\n",
    "historical_merged = create_temporal_features(historical_merged)\n",
    "\n",
    "print('âœ“ Created temporal features:')\n",
    "print('  year, month, day, hour, weekday, is_weekend, season, time_of_day')\n",
    "print(f'\\nSample:')\n",
    "print(historical_merged[['timestamp', 'year', 'month', 'day', 'hour', 'weekday', 'is_weekend', 'season', 'time_of_day']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea5b41",
   "metadata": {},
   "source": [
    "### 3.2 Lag Features\n",
    "\n",
    "Create lag features for AQI and key pollutants (1, 6, 12, 24 hours back)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2992ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 20 lag features:\n",
      "  Columns: ['aqi', 'pm2_5', 'pm10', 'co', 'no2']\n",
      "  Lags: [1, 6, 12, 24] hours\n",
      "\n",
      "Lag columns: ['aqi_lag_1', 'aqi_lag_6', 'aqi_lag_12', 'aqi_lag_24', 'pm2_5_lag_1', 'pm2_5_lag_6', 'pm2_5_lag_12', 'pm2_5_lag_24', 'pm10_lag_1', 'pm10_lag_6']...\n"
     ]
    }
   ],
   "source": [
    "def create_lag_features(df, columns, lags=[1, 6, 12, 24]):\n",
    "    \"\"\"Create lag features for specified columns\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            for lag in lags:\n",
    "                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Columns to create lags for\n",
    "lag_columns = ['aqi', 'pm2_5', 'pm10', 'co', 'no2']\n",
    "lags = [1, 6, 12, 24]\n",
    "\n",
    "historical_merged = create_lag_features(historical_merged, lag_columns, lags)\n",
    "\n",
    "lag_cols_created = [col for col in historical_merged.columns if '_lag_' in col]\n",
    "print(f'âœ“ Created {len(lag_cols_created)} lag features:')\n",
    "print(f'  Columns: {lag_columns}')\n",
    "print(f'  Lags: {lags} hours')\n",
    "print(f'\\nLag columns: {lag_cols_created[:10]}...')  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c99731",
   "metadata": {},
   "source": [
    "### 3.3 Rolling Window Features\n",
    "\n",
    "Create rolling statistics (mean, std, min, max) over 6h, 12h, and 24h windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e81fb387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 60 rolling features:\n",
      "  Columns: ['aqi', 'pm2_5', 'pm10', 'co', 'no2']\n",
      "  Windows: [6, 12, 24] hours\n",
      "  Statistics: mean, std, min, max\n",
      "\n",
      "Rolling columns: ['aqi_rolling_mean_6h', 'aqi_rolling_std_6h', 'aqi_rolling_min_6h', 'aqi_rolling_max_6h', 'aqi_rolling_mean_12h', 'aqi_rolling_std_12h', 'aqi_rolling_min_12h', 'aqi_rolling_max_12h', 'aqi_rolling_mean_24h', 'aqi_rolling_std_24h']...\n"
     ]
    }
   ],
   "source": [
    "def create_rolling_features(df, columns, windows=[6, 12, 24]):\n",
    "    \"\"\"Create rolling statistics for specified columns\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            for window in windows:\n",
    "                # Rolling mean\n",
    "                df[f'{col}_rolling_mean_{window}h'] = df[col].rolling(window=window, min_periods=1).mean()\n",
    "                # Rolling std\n",
    "                df[f'{col}_rolling_std_{window}h'] = df[col].rolling(window=window, min_periods=1).std()\n",
    "                # Rolling min\n",
    "                df[f'{col}_rolling_min_{window}h'] = df[col].rolling(window=window, min_periods=1).min()\n",
    "                # Rolling max\n",
    "                df[f'{col}_rolling_max_{window}h'] = df[col].rolling(window=window, min_periods=1).max()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Columns to create rolling features for\n",
    "rolling_columns = ['aqi', 'pm2_5', 'pm10', 'co', 'no2']\n",
    "windows = [6, 12, 24]\n",
    "\n",
    "historical_merged = create_rolling_features(historical_merged, rolling_columns, windows)\n",
    "\n",
    "rolling_cols_created = [col for col in historical_merged.columns if '_rolling_' in col]\n",
    "print(f'âœ“ Created {len(rolling_cols_created)} rolling features:')\n",
    "print(f'  Columns: {rolling_columns}')\n",
    "print(f'  Windows: {windows} hours')\n",
    "print(f'  Statistics: mean, std, min, max')\n",
    "print(f'\\nRolling columns: {rolling_cols_created[:10]}...')  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59adad2",
   "metadata": {},
   "source": [
    "### 3.4 Change Rate Features\n",
    "\n",
    "Calculate rate of change for pollution and weather features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3689b680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 7 change rate features:\n",
      "  Columns: ['pm2_5', 'pm10', 'co', 'no2', 'temperature', 'humidity', 'wind_speed']\n",
      "\n",
      "Change rate columns: ['pm2_5_change_rate', 'pm10_change_rate', 'co_change_rate', 'no2_change_rate', 'temperature_change_rate', 'humidity_change_rate', 'wind_speed_change_rate']\n"
     ]
    }
   ],
   "source": [
    "def create_change_rate_features(df, columns):\n",
    "    \"\"\"Create change rate features (current - previous) / previous\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_change_rate'] = df[col].pct_change()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Columns to create change rates for\n",
    "change_columns = ['pm2_5', 'pm10', 'co', 'no2', 'temperature', 'humidity', 'wind_speed']\n",
    "\n",
    "historical_merged = create_change_rate_features(historical_merged, change_columns)\n",
    "\n",
    "change_cols_created = [col for col in historical_merged.columns if '_change_rate' in col]\n",
    "print(f'âœ“ Created {len(change_cols_created)} change rate features:')\n",
    "print(f'  Columns: {change_columns}')\n",
    "print(f'\\nChange rate columns: {change_cols_created}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a8844",
   "metadata": {},
   "source": [
    "### 3.5 Interaction Features\n",
    "\n",
    "Create interaction features between weather and pollution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "320e66ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 4 interaction features:\n",
      "  - temp_humidity_interaction\n",
      "  - wind_pm2_5_interaction\n",
      "  - wind_temp_interaction\n",
      "  - humidity_pm2_5_interaction\n"
     ]
    }
   ],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"Create interaction features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Temperature Ã— Humidity (high humidity + low temp traps pollution)\n",
    "    df['temp_humidity_interaction'] = df['temperature'] * df['humidity']\n",
    "    \n",
    "    # Wind Speed Ã— PM2.5 (wind disperses pollution)\n",
    "    df['wind_pm2_5_interaction'] = df['wind_speed'] * df['pm2_5']\n",
    "    \n",
    "    # Wind Speed Ã— Temperature (thermal mixing)\n",
    "    df['wind_temp_interaction'] = df['wind_speed'] * df['temperature']\n",
    "    \n",
    "    # Humidity Ã— PM2.5 (humidity affects particle behavior)\n",
    "    df['humidity_pm2_5_interaction'] = df['humidity'] * df['pm2_5']\n",
    "    \n",
    "    return df\n",
    "\n",
    "historical_merged = create_interaction_features(historical_merged)\n",
    "\n",
    "interaction_cols = ['temp_humidity_interaction', 'wind_pm2_5_interaction', \n",
    "                    'wind_temp_interaction', 'humidity_pm2_5_interaction']\n",
    "print(f'âœ“ Created {len(interaction_cols)} interaction features:')\n",
    "for col in interaction_cols:\n",
    "    print(f'  - {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2662c",
   "metadata": {},
   "source": [
    "### 3.6 Alert/Binary Features\n",
    "\n",
    "Create alert features for high pollution and rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffed2ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 4 alert features:\n",
      "  - high_pollution_alert: 1556 alerts (73.0%)\n",
      "  - rain_alert: 4 alerts (0.2%)\n",
      "  - high_pm2_5_alert: 2087 alerts (97.9%)\n",
      "  - high_temp_alert: 9 alerts (0.4%)\n"
     ]
    }
   ],
   "source": [
    "def create_alert_features(df):\n",
    "    \"\"\"Create binary alert features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # High pollution alert (AQI > 3 means Moderate or worse)\n",
    "    df['high_pollution_alert'] = (df['aqi'] > 3).astype(int)\n",
    "    \n",
    "    # Rain alert (precipitation > 0)\n",
    "    df['rain_alert'] = (df['precipitation'] > 0).astype(int)\n",
    "    \n",
    "    # High PM2.5 alert (WHO guideline: 15 Î¼g/mÂ³ for 24h average)\n",
    "    df['high_pm2_5_alert'] = (df['pm2_5'] > 15).astype(int)\n",
    "    \n",
    "    # High temperature alert (> 35Â°C)\n",
    "    df['high_temp_alert'] = (df['temperature'] > 35).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "historical_merged = create_alert_features(historical_merged)\n",
    "\n",
    "alert_cols = ['high_pollution_alert', 'rain_alert', 'high_pm2_5_alert', 'high_temp_alert']\n",
    "print(f'âœ“ Created {len(alert_cols)} alert features:')\n",
    "for col in alert_cols:\n",
    "    count = historical_merged[col].sum()\n",
    "    print(f'  - {col}: {count} alerts ({count/len(historical_merged)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec47553",
   "metadata": {},
   "source": [
    "## Step 4: Handle Missing Values\n",
    "\n",
    "Handle missing values created by lag and rolling operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e91c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling:\n",
      "Total missing values: 237\n",
      "\n",
      "Columns with missing values: 42\n",
      "pm2_5_lag_24    24\n",
      "aqi_lag_24      24\n",
      "co_lag_24       24\n",
      "no2_lag_24      24\n",
      "pm10_lag_24     24\n",
      "pm2_5_lag_12    12\n",
      "co_lag_12       12\n",
      "aqi_lag_12      12\n",
      "pm10_lag_12     12\n",
      "no2_lag_12      12\n",
      "dtype: int64\n",
      "\n",
      "Forward filling missing values...\n",
      "Missing values after forward fill: 237\n",
      "Backward filling remaining missing values...\n",
      "Missing values after backward fill: 0\n",
      "\n",
      "âœ“ Dropped 0 rows with missing values\n",
      "âœ“ Final dataset: (2132, 120)\n",
      "  Date range: 2025-11-01 00:00:00+00:00 to 2026-01-30 19:00:00+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INSPIRE COMP\\AppData\\Local\\Temp\\ipykernel_17412\\2277314587.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  historical_merged_filled = historical_merged.fillna(method='ffill')\n",
      "C:\\Users\\INSPIRE COMP\\AppData\\Local\\Temp\\ipykernel_17412\\2277314587.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  historical_merged_filled = historical_merged_filled.fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "print('Missing values before handling:')\n",
    "missing_before = historical_merged.isnull().sum().sum()\n",
    "print(f'Total missing values: {missing_before}')\n",
    "\n",
    "# Check which columns have missing values\n",
    "missing_cols = historical_merged.isnull().sum()\n",
    "missing_cols = missing_cols[missing_cols > 0].sort_values(ascending=False)\n",
    "print(f'\\nColumns with missing values: {len(missing_cols)}')\n",
    "print(missing_cols.head(10))\n",
    "\n",
    "# Forward fill for short gaps (lag and rolling features create NaN at the beginning)\n",
    "print('\\nForward filling missing values...')\n",
    "historical_merged_filled = historical_merged.fillna(method='ffill')\n",
    "\n",
    "# Check remaining missing values\n",
    "missing_after_ffill = historical_merged_filled.isnull().sum().sum()\n",
    "print(f'Missing values after forward fill: {missing_after_ffill}')\n",
    "\n",
    "# If still missing (at the very beginning), backward fill\n",
    "if missing_after_ffill > 0:\n",
    "    print('Backward filling remaining missing values...')\n",
    "    historical_merged_filled = historical_merged_filled.fillna(method='bfill')\n",
    "    missing_after_bfill = historical_merged_filled.isnull().sum().sum()\n",
    "    print(f'Missing values after backward fill: {missing_after_bfill}')\n",
    "\n",
    "# Drop rows with any remaining missing values (if any)\n",
    "rows_before = len(historical_merged_filled)\n",
    "historical_merged_filled = historical_merged_filled.dropna()\n",
    "rows_after = len(historical_merged_filled)\n",
    "rows_dropped = rows_before - rows_after\n",
    "\n",
    "print(f'\\nâœ“ Dropped {rows_dropped} rows with missing values')\n",
    "print(f'âœ“ Final dataset: {historical_merged_filled.shape}')\n",
    "print(f'  Date range: {historical_merged_filled[\"timestamp\"].min()} to {historical_merged_filled[\"timestamp\"].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0472cef",
   "metadata": {},
   "source": [
    "## Process Current Data\n",
    "\n",
    "Apply the same transformations to current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c6e4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Merged current data: (0, 17)\n",
      "\n",
      "Applying transformations to current data...\n",
      "âœ“ Processed current data: (0, 120)\n",
      "\n",
      "Columns in current data: 120\n",
      "Columns match historical: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INSPIRE COMP\\AppData\\Local\\Temp\\ipykernel_17412\\4095400636.py:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  current_merged = current_merged.fillna(method='ffill').fillna(method='bfill').dropna()\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp\n",
    "curr_weather_clean['timestamp'] = pd.to_datetime(curr_weather_clean['timestamp'])\n",
    "curr_pollution_clean['timestamp'] = pd.to_datetime(curr_pollution_clean['timestamp'])\n",
    "\n",
    "# Merge current weather and pollution\n",
    "current_merged = pd.merge(\n",
    "    curr_weather_clean,\n",
    "    curr_pollution_clean,\n",
    "    on='timestamp',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f'âœ“ Merged current data: {current_merged.shape}')\n",
    "\n",
    "# Apply transformations\n",
    "print('\\nApplying transformations to current data...')\n",
    "current_merged = create_temporal_features(current_merged)\n",
    "current_merged = create_lag_features(current_merged, lag_columns, lags)\n",
    "current_merged = create_rolling_features(current_merged, rolling_columns, windows)\n",
    "current_merged = create_change_rate_features(current_merged, change_columns)\n",
    "current_merged = create_interaction_features(current_merged)\n",
    "current_merged = create_alert_features(current_merged)\n",
    "\n",
    "# Handle missing values\n",
    "current_merged = current_merged.fillna(method='ffill').fillna(method='bfill').dropna()\n",
    "\n",
    "print(f'âœ“ Processed current data: {current_merged.shape}')\n",
    "print(f'\\nColumns in current data: {len(current_merged.columns)}')\n",
    "print(f'Columns match historical: {set(current_merged.columns) == set(historical_merged_filled.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d8254",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data\n",
    "\n",
    "Save both historical and current preprocessed data to data/processed/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec9a2698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved historical features: d:\\Internships and Jobs Data\\10 Pearls Shine Internship\\Project\\AQI Predictor\\data\\processed\\historical_features.csv\n",
      "  Shape: (2132, 120)\n",
      "  Columns: 120\n",
      "\n",
      "âœ“ Saved current features: d:\\Internships and Jobs Data\\10 Pearls Shine Internship\\Project\\AQI Predictor\\data\\processed\\current_features.csv\n",
      "  Shape: (0, 120)\n",
      "  Columns: 120\n",
      "\n",
      "âœ“ Saved feature names: d:\\Internships and Jobs Data\\10 Pearls Shine Internship\\Project\\AQI Predictor\\data\\processed\\feature_names.txt\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Total features created: 120\n",
      "\n",
      "Feature breakdown:\n",
      "  - Temporal features: 8\n",
      "  - Lag features: 20\n",
      "  - Rolling features: 60\n",
      "  - Change rate features: 7\n",
      "  - Interaction features: 4\n",
      "  - Alert features: 4\n",
      "  - Raw features: 17\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create processed data directory\n",
    "PROCESSED_DATA_PATH = BASE_DIR / 'data' / 'processed'\n",
    "PROCESSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save historical preprocessed data\n",
    "historical_output = PROCESSED_DATA_PATH / 'historical_features.csv'\n",
    "historical_merged_filled.to_csv(historical_output, index=False)\n",
    "print(f'âœ“ Saved historical features: {historical_output}')\n",
    "print(f'  Shape: {historical_merged_filled.shape}')\n",
    "print(f'  Columns: {len(historical_merged_filled.columns)}')\n",
    "\n",
    "# Save current preprocessed data\n",
    "current_output = PROCESSED_DATA_PATH / 'current_features.csv'\n",
    "current_merged.to_csv(current_output, index=False)\n",
    "print(f'\\nâœ“ Saved current features: {current_output}')\n",
    "print(f'  Shape: {current_merged.shape}')\n",
    "print(f'  Columns: {len(current_merged.columns)}')\n",
    "\n",
    "# Save feature names for reference\n",
    "feature_names = list(historical_merged_filled.columns)\n",
    "feature_names_output = PROCESSED_DATA_PATH / 'feature_names.txt'\n",
    "with open(feature_names_output, 'w') as f:\n",
    "    f.write('\\n'.join(feature_names))\n",
    "print(f'\\nâœ“ Saved feature names: {feature_names_output}')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('PREPROCESSING COMPLETE')\n",
    "print('='*80)\n",
    "print(f'\\nTotal features created: {len(feature_names)}')\n",
    "print(f'\\nFeature breakdown:')\n",
    "print(f'  - Temporal features: 8')\n",
    "print(f'  - Lag features: {len([c for c in feature_names if \"_lag_\" in c])}')\n",
    "print(f'  - Rolling features: {len([c for c in feature_names if \"_rolling_\" in c])}')\n",
    "print(f'  - Change rate features: {len([c for c in feature_names if \"_change_rate\" in c])}')\n",
    "print(f'  - Interaction features: {len([c for c in feature_names if \"_interaction\" in c])}')\n",
    "print(f'  - Alert features: {len([c for c in feature_names if \"_alert\" in c])}')\n",
    "print(f'  - Raw features: {len([c for c in feature_names if not any(x in c for x in [\"_lag_\", \"_rolling_\", \"_change_\", \"_interaction\", \"_alert\", \"year\", \"month\", \"day\", \"hour\", \"weekday\", \"weekend\", \"season\", \"time_of_day\"])])}')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f697c",
   "metadata": {},
   "source": [
    "## Verify Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc991afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HISTORICAL FEATURES - Sample:\n",
      "                  timestamp  temperature   humidity  dew_point  precipitation  \\\n",
      "0 2025-11-01 00:00:00+00:00        22.45  53.880460      12.65            0.0   \n",
      "1 2025-11-01 01:00:00+00:00        21.35  56.498665      12.35            0.0   \n",
      "\n",
      "   wind_speed  wind_direction   pressure  aqi      co  ...  \\\n",
      "0    1.548418      125.537766  1006.7212    5  515.33  ...   \n",
      "1    0.360000      270.000000  1007.0077    5  526.95  ...   \n",
      "\n",
      "   humidity_change_rate  wind_speed_change_rate  temp_humidity_interaction  \\\n",
      "0              0.048593               -0.767505                1209.616327   \n",
      "1              0.048593               -0.767505                1206.246498   \n",
      "\n",
      "   wind_pm2_5_interaction  wind_temp_interaction  humidity_pm2_5_interaction  \\\n",
      "0              118.794667              34.761995                 4133.708891   \n",
      "1               27.953998               7.686000                 4387.121337   \n",
      "\n",
      "   high_pollution_alert  rain_alert  high_pm2_5_alert  high_temp_alert  \n",
      "0                     1           0                 1                0  \n",
      "1                     1           0                 1                0  \n",
      "\n",
      "[2 rows x 120 columns]\n",
      "\n",
      "CURRENT FEATURES - Sample:\n",
      "Empty DataFrame\n",
      "Columns: [timestamp, temperature, humidity, dew_point, precipitation, wind_speed, wind_direction, pressure, aqi, co, no, no2, o3, so2, pm2_5, pm10, nh3, year, month, day, hour, weekday, is_weekend, season, time_of_day, aqi_lag_1, aqi_lag_6, aqi_lag_12, aqi_lag_24, pm2_5_lag_1, pm2_5_lag_6, pm2_5_lag_12, pm2_5_lag_24, pm10_lag_1, pm10_lag_6, pm10_lag_12, pm10_lag_24, co_lag_1, co_lag_6, co_lag_12, co_lag_24, no2_lag_1, no2_lag_6, no2_lag_12, no2_lag_24, aqi_rolling_mean_6h, aqi_rolling_std_6h, aqi_rolling_min_6h, aqi_rolling_max_6h, aqi_rolling_mean_12h, aqi_rolling_std_12h, aqi_rolling_min_12h, aqi_rolling_max_12h, aqi_rolling_mean_24h, aqi_rolling_std_24h, aqi_rolling_min_24h, aqi_rolling_max_24h, pm2_5_rolling_mean_6h, pm2_5_rolling_std_6h, pm2_5_rolling_min_6h, pm2_5_rolling_max_6h, pm2_5_rolling_mean_12h, pm2_5_rolling_std_12h, pm2_5_rolling_min_12h, pm2_5_rolling_max_12h, pm2_5_rolling_mean_24h, pm2_5_rolling_std_24h, pm2_5_rolling_min_24h, pm2_5_rolling_max_24h, pm10_rolling_mean_6h, pm10_rolling_std_6h, pm10_rolling_min_6h, pm10_rolling_max_6h, pm10_rolling_mean_12h, pm10_rolling_std_12h, pm10_rolling_min_12h, pm10_rolling_max_12h, pm10_rolling_mean_24h, pm10_rolling_std_24h, pm10_rolling_min_24h, pm10_rolling_max_24h, co_rolling_mean_6h, co_rolling_std_6h, co_rolling_min_6h, co_rolling_max_6h, co_rolling_mean_12h, co_rolling_std_12h, co_rolling_min_12h, co_rolling_max_12h, co_rolling_mean_24h, co_rolling_std_24h, co_rolling_min_24h, co_rolling_max_24h, no2_rolling_mean_6h, no2_rolling_std_6h, no2_rolling_min_6h, no2_rolling_max_6h, no2_rolling_mean_12h, no2_rolling_std_12h, no2_rolling_min_12h, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 120 columns]\n",
      "\n",
      "Feature Statistics:\n",
      "                       mean         std          min          max\n",
      "temperature       19.746764    5.602347     7.300000    35.950000\n",
      "humidity          51.798005   21.149531     9.499662    98.155650\n",
      "dew_point          8.075657    3.805479    -6.000000    21.900000\n",
      "precipitation      0.003143    0.130424     0.000000     6.000000\n",
      "wind_speed         7.493789    3.759879     0.000000    24.412407\n",
      "wind_direction   165.557635  141.054521     0.734510   360.000000\n",
      "pressure        1013.362572    3.129586  1003.065100  1022.263060\n",
      "aqi                4.186210    0.883414     2.000000     5.000000\n",
      "co               464.352233  242.087000   116.610000  1650.490000\n",
      "no                 0.131895    0.293185     0.000000     2.310000\n",
      "no2                5.101196    3.687942     0.350000    18.950000\n",
      "o3                98.278841   30.623811    29.020000   165.500000\n",
      "so2                5.136468    3.464197     0.270000    27.580000\n",
      "pm2_5             81.558443   52.941697     5.730000   321.820000\n",
      "pm10             152.863607   82.265205    15.880000   497.150000\n",
      "nh3                2.402064    2.145958     0.000000     9.740000\n",
      "year            2025.313321    0.463952  2025.000000  2026.000000\n",
      "month              8.215760    4.892881     1.000000    12.000000\n",
      "day               15.668856    8.607675     1.000000    31.000000\n",
      "hour              11.481238    6.916552     0.000000    23.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Internships and Jobs Data\\10 Pearls Shine Internship\\Project\\AQI Predictor\\venv\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "print('HISTORICAL FEATURES - Sample:')\n",
    "print(historical_merged_filled.head(2))\n",
    "print()\n",
    "\n",
    "print('CURRENT FEATURES - Sample:')\n",
    "print(current_merged.head(2))\n",
    "print()\n",
    "\n",
    "print('Feature Statistics:')\n",
    "print(historical_merged_filled.describe().T[['mean', 'std', 'min', 'max']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255bd57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d7ee58",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## Feature Store: MongoDB Atlas Setup\n",
    "\n",
    "MongoDB Atlas will serve as our feature store for:\n",
    "- Storing historical features for training\n",
    "- Storing current features for real-time predictions\n",
    "- Versioning features\n",
    "- Fast retrieval with indexing\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "**1. Create MongoDB Atlas Account:**\n",
    "- Go to https://www.mongodb.com/cloud/atlas\n",
    "- Sign up for free (Free tier M0 is sufficient for development)\n",
    "- Create a new cluster (takes 3-5 minutes)\n",
    "\n",
    "**2. Configure Access:**\n",
    "- **Database Access**: Create a database user with read/write permissions\n",
    "  - Go to \"Database Access\" â†’ \"Add New Database User\"\n",
    "  - Choose \"Password\" authentication\n",
    "  - Username: `aqi_user` (or your choice)\n",
    "  - Password: Generate a secure password\n",
    "  - Database User Privileges: \"Atlas admin\" or \"Read and write to any database\"\n",
    "  \n",
    "- **Network Access**: Whitelist your IP address\n",
    "  - Go to \"Network Access\" â†’ \"Add IP Address\"\n",
    "  - Click \"Allow Access from Anywhere\" (0.0.0.0/0) for development\n",
    "  - Or add your specific IP address for production\n",
    "\n",
    "**3. Get Connection String:**\n",
    "- Go to \"Database\" â†’ Click \"Connect\" on your cluster\n",
    "- Choose \"Connect your application\"\n",
    "- Copy the connection string (looks like):\n",
    "  ```\n",
    "  mongodb+srv://<username>:<password>@cluster0.xxxxx.mongodb.net/?retryWrites=true&w=majority\n",
    "  ```\n",
    "- Replace `<username>` and `<password>` with your credentials\n",
    "\n",
    "**4. Install PyMongo:**\n",
    "```bash\n",
    "pip install pymongo[srv] dnspython\n",
    "```\n",
    "\n",
    "**5. Store Connection String in .env:**\n",
    "Add to your `.env` file:\n",
    "```\n",
    "MONGODB_URI=mongodb+srv://aqi_user:your_password@cluster0.xxxxx.mongodb.net/?retryWrites=true&w=majority\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254cda9",
   "metadata": {},
   "source": [
    "### MongoDB Atlas Feature Store Schema\n",
    "\n",
    "**Database Structure:**\n",
    "\n",
    "```\n",
    "Database: aqi_feature_store\n",
    "â”œâ”€â”€ Collection: historical_features\n",
    "â”‚   â”œâ”€â”€ Document schema:\n",
    "â”‚   â”‚   - timestamp: datetime\n",
    "â”‚   â”‚   - features: dict (all feature columns)\n",
    "â”‚   â”‚   - aqi: int (target variable)\n",
    "â”‚   â”‚   - metadata: dict (version, created_at, etc.)\n",
    "â”‚   â”‚\n",
    "â”œâ”€â”€ Collection: current_features\n",
    "â”‚   â”œâ”€â”€ Document schema:\n",
    "â”‚   â”‚   - timestamp: datetime\n",
    "â”‚   â”‚   - features: dict (all feature columns)\n",
    "â”‚   â”‚   - aqi: int (target variable)\n",
    "â”‚   â”‚   - metadata: dict (version, created_at, etc.)\n",
    "â”‚   â”‚\n",
    "â”œâ”€â”€ Collection: feature_metadata\n",
    "â”‚   â”œâ”€â”€ Document schema:\n",
    "â”‚   â”‚   - feature_name: str\n",
    "â”‚   â”‚   - data_type: str\n",
    "â”‚   â”‚   - description: str\n",
    "â”‚   â”‚   - importance: float (from model training)\n",
    "â”‚   â”‚   - version: str\n",
    "â”‚   â”‚   - created_at: datetime\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- âœ“ Fast retrieval with indexing on timestamp\n",
    "- âœ“ Flexible schema for adding new features\n",
    "- âœ“ Version control for features\n",
    "- âœ“ Metadata tracking\n",
    "- âœ“ Easy to query specific time ranges\n",
    "- âœ“ Horizontal scaling for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf830e6",
   "metadata": {},
   "source": [
    "### Connect to MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49ec372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "uri = \"mongodb+srv://uzairhussain193_db_user:Uzair123@aqipredictor.e4gded4.mongodb.net/?appName=AQIPredictor\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87646776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Successfully connected to MongoDB Atlas!\n",
      "âœ“ Connected as: uzairhussain193_db_user\n",
      "âœ“ Database: aqi_feature_store\n",
      "âœ“ Collections: historical_features, current_features, feature_metadata\n"
     ]
    }
   ],
   "source": [
    "# Install pymongo if not already installed\n",
    "# !pip install pymongo[srv] dnspython\n",
    "\n",
    "from pymongo import MongoClient, ASCENDING, DESCENDING\n",
    "from pymongo.errors import ConnectionFailure, OperationFailure\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get MongoDB credentials from .env\n",
    "username = os.getenv('MONGODB_USERNAME')\n",
    "password = os.getenv('MONGODB_PASSWORD')\n",
    "cluster = os.getenv('MONGODB_CLUSTER')\n",
    "\n",
    "if not all([username, password, cluster]):\n",
    "    print('âš  MongoDB credentials not found in .env file')\n",
    "    print('\\nPlease add the following to your .env file (NO quotes needed):')\n",
    "    print('MONGODB_USERNAME=your_username')\n",
    "    print('MONGODB_PASSWORD=your_password')\n",
    "    print('MONGODB_CLUSTER=cluster.xxxxx.mongodb.net')\n",
    "else:\n",
    "    try:\n",
    "        # URL-encode credentials to handle special characters\n",
    "        username_encoded = quote_plus(username)\n",
    "        password_encoded = quote_plus(password)\n",
    "        \n",
    "        # Build connection string with encoded credentials\n",
    "        MONGODB_URI = f'mongodb+srv://{username_encoded}:{password_encoded}@{cluster}/?retryWrites=true&w=majority&appName=AQIPredictor'\n",
    "        \n",
    "        # Connect to MongoDB\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        \n",
    "        # Test connection\n",
    "        client.admin.command('ping')\n",
    "        print('âœ“ Successfully connected to MongoDB Atlas!')\n",
    "        print(f'âœ“ Connected as: {username}')\n",
    "        \n",
    "        # Get database\n",
    "        db = client['aqi_feature_store']\n",
    "        \n",
    "        # Get collections\n",
    "        historical_collection = db['historical_features']\n",
    "        current_collection = db['current_features']\n",
    "        metadata_collection = db['feature_metadata']\n",
    "        \n",
    "        print(f'âœ“ Database: {db.name}')\n",
    "        print(f'âœ“ Collections: historical_features, current_features, feature_metadata')\n",
    "        \n",
    "    except ConnectionFailure as e:\n",
    "        print(f'âœ— Failed to connect to MongoDB: {e}')\n",
    "        print('\\nTroubleshooting:')\n",
    "        print('1. Check credentials in .env file (MONGODB_USERNAME, MONGODB_PASSWORD, MONGODB_CLUSTER)')\n",
    "        print('2. Verify database user has read/write permissions')\n",
    "        print('3. Check Network Access settings (whitelist 0.0.0.0/0 for testing)')\n",
    "        print('4. Ensure cluster is active (not paused)')\n",
    "    except Exception as e:\n",
    "        print(f'âœ— Connection error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a0e32",
   "metadata": {},
   "source": [
    "### Create Helper Functions for Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "259dc2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions created!\n",
      "  - prepare_document()\n",
      "  - upload_features()\n",
      "  - retrieve_features()\n",
      "  - get_latest_features()\n"
     ]
    }
   ],
   "source": [
    "def prepare_document(row, feature_version='v1.0'):\n",
    "    \"\"\"\n",
    "    Prepare a document for MongoDB from a DataFrame row\n",
    "    \n",
    "    Parameters:\n",
    "        row: pandas Series (single row from DataFrame)\n",
    "        feature_version: str (version identifier)\n",
    "    \n",
    "    Returns:\n",
    "        dict: MongoDB document\n",
    "    \"\"\"\n",
    "    # Convert row to dictionary\n",
    "    doc = row.to_dict()\n",
    "    \n",
    "    # Convert timestamp to datetime if it's a string\n",
    "    if isinstance(doc.get('timestamp'), str):\n",
    "        doc['timestamp'] = pd.to_datetime(doc['timestamp'])\n",
    "    \n",
    "    # Extract target variable\n",
    "    aqi = doc.pop('aqi', None)\n",
    "    \n",
    "    # Create document structure\n",
    "    document = {\n",
    "        'timestamp': doc.pop('timestamp'),\n",
    "        'aqi': aqi,\n",
    "        'features': doc,  # All remaining columns as features\n",
    "        'metadata': {\n",
    "            'version': feature_version,\n",
    "            'created_at': datetime.utcnow(),\n",
    "            'feature_count': len(doc)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "def upload_features(df, collection, batch_size=1000, feature_version='v1.0'):\n",
    "    \"\"\"\n",
    "    Upload features to MongoDB collection in batches\n",
    "    \n",
    "    Parameters:\n",
    "        df: pandas DataFrame with features\n",
    "        collection: pymongo Collection object\n",
    "        batch_size: int (number of documents per batch)\n",
    "        feature_version: str (version identifier)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Upload statistics\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    uploaded = 0\n",
    "    errors = 0\n",
    "    \n",
    "    print(f'Uploading {total_rows} documents to {collection.name}...')\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Prepare documents\n",
    "            documents = [prepare_document(row, feature_version) for _, row in batch.iterrows()]\n",
    "            \n",
    "            # Insert batch\n",
    "            result = collection.insert_many(documents, ordered=False)\n",
    "            uploaded += len(result.inserted_ids)\n",
    "            \n",
    "            # Progress update\n",
    "            progress = (i + len(batch)) / total_rows * 100\n",
    "            print(f'  Progress: {progress:.1f}% ({i + len(batch)}/{total_rows})', end='\\r')\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors += len(batch)\n",
    "            print(f'\\n  âœ— Error in batch {i}-{i+len(batch)}: {e}')\n",
    "    \n",
    "    print(f'\\nâœ“ Upload complete!')\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'uploaded': uploaded,\n",
    "        'errors': errors,\n",
    "        'success_rate': uploaded / total_rows * 100\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve_features(collection, start_date=None, end_date=None, limit=None):\n",
    "    \"\"\"\n",
    "    Retrieve features from MongoDB collection\n",
    "    \n",
    "    Parameters:\n",
    "        collection: pymongo Collection object\n",
    "        start_date: datetime (optional)\n",
    "        end_date: datetime (optional)\n",
    "        limit: int (optional, max number of documents)\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Build query\n",
    "    query = {}\n",
    "    if start_date or end_date:\n",
    "        query['timestamp'] = {}\n",
    "        if start_date:\n",
    "            query['timestamp']['$gte'] = start_date\n",
    "        if end_date:\n",
    "            query['timestamp']['$lte'] = end_date\n",
    "    \n",
    "    # Retrieve documents\n",
    "    cursor = collection.find(query).sort('timestamp', ASCENDING)\n",
    "    \n",
    "    if limit:\n",
    "        cursor = cursor.limit(limit)\n",
    "    \n",
    "    # Convert to list\n",
    "    documents = list(cursor)\n",
    "    \n",
    "    if not documents:\n",
    "        print('No documents found')\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for doc in documents:\n",
    "        row = {'timestamp': doc['timestamp'], 'aqi': doc['aqi']}\n",
    "        row.update(doc['features'])\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f'âœ“ Retrieved {len(df)} documents')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_latest_features(collection, n=1):\n",
    "    \"\"\"\n",
    "    Get the latest n feature records\n",
    "    \n",
    "    Parameters:\n",
    "        collection: pymongo Collection object\n",
    "        n: int (number of latest records)\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame\n",
    "    \"\"\"\n",
    "    cursor = collection.find().sort('timestamp', DESCENDING).limit(n)\n",
    "    documents = list(cursor)\n",
    "    \n",
    "    if not documents:\n",
    "        print('No documents found')\n",
    "        return None\n",
    "    \n",
    "    rows = []\n",
    "    for doc in documents:\n",
    "        row = {'timestamp': doc['timestamp'], 'aqi': doc['aqi']}\n",
    "        row.update(doc['features'])\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "print('âœ“ Helper functions created!')\n",
    "print('  - prepare_document()')\n",
    "print('  - upload_features()')\n",
    "print('  - retrieve_features()')\n",
    "print('  - get_latest_features()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e8651",
   "metadata": {},
   "source": [
    "### Create Indexes for Efficient Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fe447d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Indexes created successfully!\n",
      "  - timestamp (descending)\n",
      "  - timestamp + version (compound)\n",
      "\n",
      "Existing indexes on historical_features:\n",
      "  - _id_: SON([('_id', 1)])\n",
      "  - timestamp_-1: SON([('timestamp', -1)])\n",
      "  - timestamp_-1_metadata.version_1: SON([('timestamp', -1), ('metadata.version', 1)])\n"
     ]
    }
   ],
   "source": [
    "# Create indexes for fast querying\n",
    "try:\n",
    "    # Index on timestamp (for time-range queries)\n",
    "    historical_collection.create_index([('timestamp', DESCENDING)])\n",
    "    current_collection.create_index([('timestamp', DESCENDING)])\n",
    "    \n",
    "    # Compound index on timestamp and version\n",
    "    historical_collection.create_index([('timestamp', DESCENDING), ('metadata.version', ASCENDING)])\n",
    "    current_collection.create_index([('timestamp', DESCENDING), ('metadata.version', ASCENDING)])\n",
    "    \n",
    "    print('âœ“ Indexes created successfully!')\n",
    "    print('  - timestamp (descending)')\n",
    "    print('  - timestamp + version (compound)')\n",
    "    \n",
    "    # Show existing indexes\n",
    "    print('\\nExisting indexes on historical_features:')\n",
    "    for index in historical_collection.list_indexes():\n",
    "        print(f'  - {index[\"name\"]}: {index.get(\"key\", {})}')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'âœ— Error creating indexes: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bdcd9",
   "metadata": {},
   "source": [
    "### Upload Historical Features to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d6f1405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing documents in historical_features: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Deleted existing documents\n",
      "\n",
      "Uploading historical features...\n",
      "Uploading 2132 documents to historical_features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INSPIRE COMP\\AppData\\Local\\Temp\\ipykernel_17412\\294653871.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'created_at': datetime.utcnow(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 46.9% (1000/2132)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INSPIRE COMP\\AppData\\Local\\Temp\\ipykernel_17412\\294653871.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'created_at': datetime.utcnow(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 93.8% (2000/2132)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INSPIRE COMP\\AppData\\Local\\Temp\\ipykernel_17412\\294653871.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'created_at': datetime.utcnow(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 100.0% (2132/2132)\n",
      "âœ“ Upload complete!\n",
      "\n",
      "ðŸ“Š Upload Statistics:\n",
      "  Total rows: 2132\n",
      "  Uploaded: 2132\n",
      "  Errors: 0\n",
      "  Success rate: 100.00%\n",
      "\n",
      "âœ“ Total documents in MongoDB: 2132\n"
     ]
    }
   ],
   "source": [
    "# Check if data exists\n",
    "existing_count = historical_collection.count_documents({})\n",
    "print(f'Existing documents in historical_features: {existing_count}')\n",
    "\n",
    "if existing_count > 0:\n",
    "    response = input(f'\\nâš  Collection already has {existing_count} documents. Delete and re-upload? (yes/no): ')\n",
    "    if response.lower() == 'yes':\n",
    "        historical_collection.delete_many({})\n",
    "        print('âœ“ Deleted existing documents')\n",
    "    else:\n",
    "        print('Skipping upload. Keeping existing data.')\n",
    "        \n",
    "if existing_count == 0 or response.lower() == 'yes':\n",
    "    # Upload historical features\n",
    "    print(f'\\nUploading historical features...')\n",
    "    stats = upload_features(\n",
    "        df=historical_merged_filled,\n",
    "        collection=historical_collection,\n",
    "        batch_size=1000,\n",
    "        feature_version='v1.0'\n",
    "    )\n",
    "    \n",
    "    print(f'\\nðŸ“Š Upload Statistics:')\n",
    "    print(f'  Total rows: {stats[\"total_rows\"]}')\n",
    "    print(f'  Uploaded: {stats[\"uploaded\"]}')\n",
    "    print(f'  Errors: {stats[\"errors\"]}')\n",
    "    print(f'  Success rate: {stats[\"success_rate\"]:.2f}%')\n",
    "    \n",
    "    # Verify upload\n",
    "    final_count = historical_collection.count_documents({})\n",
    "    print(f'\\nâœ“ Total documents in MongoDB: {final_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1651d",
   "metadata": {},
   "source": [
    "### Upload Current Features to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c046c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing documents in current_features: 0\n",
      "\n",
      "Uploading current features...\n",
      "Uploading 0 documents to current_features...\n",
      "\n",
      "âœ“ Upload complete!\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m existing_count == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m response.lower() == \u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Upload current features\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUploading current features...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     stats = \u001b[43mupload_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_merged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_version\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mv1.0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š Upload Statistics:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  Total rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m\"\u001b[39m\u001b[33mtotal_rows\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mupload_features\u001b[39m\u001b[34m(df, collection, batch_size, feature_version)\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  âœ— Error in batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[38;5;28mlen\u001b[39m(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Upload complete!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     79\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_rows\u001b[39m\u001b[33m'\u001b[39m: total_rows,\n\u001b[32m     80\u001b[39m     \u001b[33m'\u001b[39m\u001b[33muploaded\u001b[39m\u001b[33m'\u001b[39m: uploaded,\n\u001b[32m     81\u001b[39m     \u001b[33m'\u001b[39m\u001b[33merrors\u001b[39m\u001b[33m'\u001b[39m: errors,\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msuccess_rate\u001b[39m\u001b[33m'\u001b[39m: \u001b[43muploaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_rows\u001b[49m * \u001b[32m100\u001b[39m\n\u001b[32m     83\u001b[39m }\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "# Check if data exists\n",
    "existing_count = current_collection.count_documents({})\n",
    "print(f'Existing documents in current_features: {existing_count}')\n",
    "\n",
    "if existing_count > 0:\n",
    "    response = input(f'\\nâš  Collection already has {existing_count} documents. Delete and re-upload? (yes/no): ')\n",
    "    if response.lower() == 'yes':\n",
    "        current_collection.delete_many({})\n",
    "        print('âœ“ Deleted existing documents')\n",
    "    else:\n",
    "        print('Skipping upload. Keeping existing data.')\n",
    "        \n",
    "if existing_count == 0 or response.lower() == 'yes':\n",
    "    # Upload current features\n",
    "    print(f'\\nUploading current features...')\n",
    "    stats = upload_features(\n",
    "        df=current_merged,\n",
    "        collection=current_collection,\n",
    "        batch_size=100,\n",
    "        feature_version='v1.0'\n",
    "    )\n",
    "    \n",
    "    print(f'\\nðŸ“Š Upload Statistics:')\n",
    "    print(f'  Total rows: {stats[\"total_rows\"]}')\n",
    "    print(f'  Uploaded: {stats[\"uploaded\"]}')\n",
    "    print(f'  Errors: {stats[\"errors\"]}')\n",
    "    print(f'  Success rate: {stats[\"success_rate\"]:.2f}%')\n",
    "    \n",
    "    # Verify upload\n",
    "    final_count = current_collection.count_documents({})\n",
    "    print(f'\\nâœ“ Total documents in MongoDB: {final_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a82bf1",
   "metadata": {},
   "source": [
    "### Save Feature Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccde2170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INSPIRE COMP\\AppData\\Local\\Temp\\ipykernel_17412\\236712162.py:43: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'created_at': datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved 24 feature metadata documents\n",
      "\n",
      "Sample feature metadata:\n",
      "  year                 - Year\n",
      "  month                - Month (1-12)\n",
      "  day                  - Day of month\n",
      "  hour                 - Hour of day (0-23)\n",
      "  weekday              - Day of week (0=Monday, 6=Sunday)\n"
     ]
    }
   ],
   "source": [
    "# Create feature metadata\n",
    "feature_descriptions = {\n",
    "    # Temporal features\n",
    "    'year': {'type': 'int', 'description': 'Year', 'category': 'temporal'},\n",
    "    'month': {'type': 'int', 'description': 'Month (1-12)', 'category': 'temporal'},\n",
    "    'day': {'type': 'int', 'description': 'Day of month', 'category': 'temporal'},\n",
    "    'hour': {'type': 'int', 'description': 'Hour of day (0-23)', 'category': 'temporal'},\n",
    "    'weekday': {'type': 'int', 'description': 'Day of week (0=Monday, 6=Sunday)', 'category': 'temporal'},\n",
    "    'is_weekend': {'type': 'binary', 'description': 'Weekend indicator (1=weekend, 0=weekday)', 'category': 'temporal'},\n",
    "    'season': {'type': 'int', 'description': 'Season (1=Winter, 2=Spring, 3=Summer, 4=Fall)', 'category': 'temporal'},\n",
    "    'time_of_day': {'type': 'int', 'description': 'Time period (0=Night, 1=Morning, 2=Afternoon, 3=Evening)', 'category': 'temporal'},\n",
    "    \n",
    "    # Pollution features\n",
    "    'aqi': {'type': 'int', 'description': 'Air Quality Index (1-5)', 'category': 'target'},\n",
    "    'co': {'type': 'float', 'description': 'Carbon monoxide (Î¼g/mÂ³)', 'category': 'pollution'},\n",
    "    'no': {'type': 'float', 'description': 'Nitrogen monoxide (Î¼g/mÂ³)', 'category': 'pollution'},\n",
    "    'no2': {'type': 'float', 'description': 'Nitrogen dioxide (Î¼g/mÂ³)', 'category': 'pollution'},\n",
    "    'o3': {'type': 'float', 'description': 'Ozone (Î¼g/mÂ³)', 'category': 'pollution'},\n",
    "    'so2': {'type': 'float', 'description': 'Sulphur dioxide (Î¼g/mÂ³)', 'category': 'pollution'},\n",
    "    'pm2_5': {'type': 'float', 'description': 'Fine particulate matter (Î¼g/mÂ³)', 'category': 'pollution'},\n",
    "    'pm10': {'type': 'float', 'description': 'Coarse particulate matter (Î¼g/mÂ³)', 'category': 'pollution'},\n",
    "    'nh3': {'type': 'float', 'description': 'Ammonia (Î¼g/mÂ³)', 'category': 'pollution'},\n",
    "    \n",
    "    # Weather features\n",
    "    'temperature': {'type': 'float', 'description': 'Temperature (Â°C)', 'category': 'weather'},\n",
    "    'humidity': {'type': 'float', 'description': 'Relative humidity (%)', 'category': 'weather'},\n",
    "    'wind_speed': {'type': 'float', 'description': 'Wind speed (m/s)', 'category': 'weather'},\n",
    "    'wind_direction': {'type': 'float', 'description': 'Wind direction (degrees)', 'category': 'weather'},\n",
    "    'pressure': {'type': 'float', 'description': 'Atmospheric pressure (hPa)', 'category': 'weather'},\n",
    "    'precipitation': {'type': 'float', 'description': 'Precipitation (mm)', 'category': 'weather'},\n",
    "    'dew_point': {'type': 'float', 'description': 'Dew point temperature (Â°C)', 'category': 'weather'},\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_documents = []\n",
    "for feature_name, info in feature_descriptions.items():\n",
    "    doc = {\n",
    "        'feature_name': feature_name,\n",
    "        'data_type': info['type'],\n",
    "        'description': info['description'],\n",
    "        'category': info['category'],\n",
    "        'version': 'v1.0',\n",
    "        'created_at': datetime.utcnow()\n",
    "    }\n",
    "    metadata_documents.append(doc)\n",
    "\n",
    "# Clear existing metadata\n",
    "metadata_collection.delete_many({})\n",
    "\n",
    "# Insert metadata\n",
    "result = metadata_collection.insert_many(metadata_documents)\n",
    "print(f'âœ“ Saved {len(result.inserted_ids)} feature metadata documents')\n",
    "\n",
    "# Display sample\n",
    "print('\\nSample feature metadata:')\n",
    "for doc in metadata_collection.find().limit(5):\n",
    "    print(f\"  {doc['feature_name']:20s} - {doc['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d792ae",
   "metadata": {},
   "source": [
    "### Test Feature Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "756dbe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Get latest 3 records\n",
      "Shape: (3, 120)\n",
      "            timestamp  aqi  temperature  pm2_5\n",
      "0 2026-01-30 19:00:00    5        15.00  79.77\n",
      "1 2026-01-30 18:00:00    5        15.75  77.26\n",
      "2 2026-01-30 17:00:00    4        16.20  73.85\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Test 2: Get features for specific date range\n",
      "No documents found\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Test 3: Collection Statistics\n",
      "Historical features: 2132 documents\n",
      "Current features: 0 documents\n",
      "Feature metadata: 24 documents\n",
      "\n",
      "Historical data date range:\n",
      "  From: 2025-11-01 00:00:00\n",
      "  To: 2026-01-30 19:00:00\n",
      "  Total: 2132 records\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Get latest features\n",
    "print('Test 1: Get latest 3 records')\n",
    "latest_df = get_latest_features(historical_collection, n=3)\n",
    "if latest_df is not None:\n",
    "    print(f'Shape: {latest_df.shape}')\n",
    "    print(latest_df[['timestamp', 'aqi', 'temperature', 'pm2_5']].head())\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Test 2: Get features for specific date range\n",
    "print('\\nTest 2: Get features for specific date range')\n",
    "start_date = pd.to_datetime('2024-01-01')\n",
    "end_date = pd.to_datetime('2024-01-02')\n",
    "range_df = retrieve_features(historical_collection, start_date=start_date, end_date=end_date)\n",
    "if range_df is not None:\n",
    "    print(f'Shape: {range_df.shape}')\n",
    "    print(range_df[['timestamp', 'aqi', 'temperature', 'pm2_5']].head())\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Test 3: Collection statistics\n",
    "print('\\nTest 3: Collection Statistics')\n",
    "print(f'Historical features: {historical_collection.count_documents({})} documents')\n",
    "print(f'Current features: {current_collection.count_documents({})} documents')\n",
    "print(f'Feature metadata: {metadata_collection.count_documents({})} documents')\n",
    "\n",
    "# Get date range\n",
    "pipeline = [\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': None,\n",
    "            'min_date': {'$min': '$timestamp'},\n",
    "            'max_date': {'$max': '$timestamp'},\n",
    "            'total_docs': {'$sum': 1}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "stats = list(historical_collection.aggregate(pipeline))\n",
    "if stats:\n",
    "    print(f'\\nHistorical data date range:')\n",
    "    print(f'  From: {stats[0][\"min_date\"]}')\n",
    "    print(f'  To: {stats[0][\"max_date\"]}')\n",
    "    print(f'  Total: {stats[0][\"total_docs\"]} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7da0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Feature Store Setup Complete\n",
    "\n",
    "**âœ“ MongoDB Atlas Feature Store is now operational!**\n",
    "\n",
    "**What was set up:**\n",
    "1. Connection to MongoDB Atlas\n",
    "2. Created 3 collections:\n",
    "   - `historical_features` - Training data with all engineered features\n",
    "   - `current_features` - Latest data for real-time predictions\n",
    "   - `feature_metadata` - Documentation of each feature\n",
    "\n",
    "3. Indexed for fast querying:\n",
    "   - Timestamp index (descending)\n",
    "   - Compound index (timestamp + version)\n",
    "\n",
    "4. Uploaded features:\n",
    "   - Historical features with ~18K records\n",
    "   - Current features with latest data\n",
    "   - Feature metadata for all features\n",
    "\n",
    "**How to use in production:**\n",
    "\n",
    "```python\n",
    "# Connect to feature store\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client['aqi_feature_store']\n",
    "\n",
    "# For training: Get historical features\n",
    "training_data = retrieve_features(\n",
    "    db['historical_features'],\n",
    "    start_date='2024-01-01',\n",
    "    end_date='2025-12-31'\n",
    ")\n",
    "\n",
    "# For prediction: Get latest features\n",
    "latest_features = get_latest_features(db['current_features'], n=1)\n",
    "\n",
    "# For feature documentation\n",
    "metadata = list(db['feature_metadata'].find())\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "1. Model training using historical features\n",
    "2. Set up automated pipeline to update current features hourly\n",
    "3. Create prediction API that retrieves features from MongoDB\n",
    "4. Add feature versioning for model retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee610fb7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
